{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Part 2:  Amazon fine foods reviews\n",
    "\n",
    " ***By:*** *Vaida Gulbinskaite and Gediminas Sadaunykas*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "For the second part of the coursework we will use the Amazon Fine Food Reviews dataset which contains 568,454 food reviews that Amazon.com users left between October 1999 and October 2012. We decided to select this dataset because it offers a wide range of venues that cuould be explored: from building product recommendation models to sentiment analysis. This dataset has been widely used by professionals and aspiring data scientists, making it largely compelling. The vast length of the dataset, makes it a viable 'playground' for big data model building.\n",
    "\n",
    "We have decided to solve text classification task. We will try to predict whether review was helpful or unhelpful to other buyers on Amazon, and what words are distinctive for helpful and unhelpful reviews. In order to do so, we have created a new metric called 'Helpfulness_perct' which is a percentage of helpfulness of the review, derived from HelpfulnessNumerator (number of people who voted the review to be helpful) and HelpfulnessDenominator (number of people who 'reviewed' the review). Helpfulness_perct was converted into two binary classes:  1 for  helpful reviews (reviews with Helpfulness_perct>=75%) 0 for unhelpful reviews (reviews with Helpfulness_perct<=25%). \n",
    "\n",
    "First, data was preprocessed via data type initialization, outlier treatment, balancing (due to 87% of classes being labeled positive). Then two data analysis pipelines, were implemented. One on single training/validation (80%/20%) split, and single combination of parameters, other on 10-fold cross validation , with parameter grid, consisting of 27 possible parameter combinations. Models were evaluated, using ROC-AUC, precision, recall, f1 scores. Finally, most significant words were found. They were defined by the total sum of IDF.TF values, adjusted for stopwords and common words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data into DataFrame\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "raw_DF = spark.read.csv(\"hdfs://saltdean/data/reviews/Reviews.csv\", header=True, inferSchema=True) # Read the data into Data Frame\n",
    "\n",
    "#Changing type of data from string (which is set by default) to float\n",
    "changedTypedf = raw_DF.withColumn(\"HelpfulnessNumerator\", raw_DF[\"HelpfulnessNumerator\"].cast(FloatType()))\n",
    "changedTypedf = changedTypedf.withColumn(\"Score\", changedTypedf[\"Score\"].cast(FloatType()))\n",
    "changedTypedf = changedTypedf.withColumn(\"HelpfulnessDenominator\", changedTypedf[\"HelpfulnessDenominator\"].cast(FloatType()))\n",
    "\n",
    "#Creating a new column that holds helpfulness percentage\n",
    "review_df = changedTypedf.withColumn(\"Helpfulness_perct\", lit((changedTypedf['HelpfulnessNumerator']/changedTypedf\n",
    "                                                           ['HelpfulnessDenominator'])*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of instances: 24892 \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## OUTLIER TREATMENT\n",
    "\n",
    "# Fill Null with 0, because division by 0 is faulty.\n",
    "review_df = review_df.na.fill(0)\n",
    "\n",
    "#Outlier definition 1: Helpfulness_perct>100% (2 rows)\n",
    "#Outlier definition 2: low visibility or helpfulnessDenominator < 10.\n",
    "review_df = review_df[(review_df['Helpfulness_perct']<=100) & (review_df['HelpfulnessDenominator']>=10)]\n",
    "\n",
    "print('Total number of instances: %d ' % review_df.count())\n",
    "print('-'*100)\n",
    "\n",
    "## (NO MISSING VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of instances: 19752\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Keep only rows for which we will have labels\n",
    "\n",
    "review_df = review_df[(review_df['Helpfulness_perct']>=75) | (review_df['Helpfulness_perct']<=25)]\n",
    "\n",
    "print('Total number of instances: %d' % review_df.count())\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Helpfulness_label|count|\n",
      "+-----------------+-----+\n",
      "|                0| 2757|\n",
      "|                1|16995|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create the labels. (>75% helpfull, <25% unhelpfull)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the UDF, that creates the labels\n",
    "def label(helpfulness):\n",
    "    label=0\n",
    "    if helpfulness >= 75:\n",
    "        label1=1\n",
    "    else:\n",
    "        label1=0\n",
    "    return label1\n",
    "\n",
    "labelUDF = udf(label, IntegerType())  # Create the UDF\n",
    "review_df = review_df.withColumn('Helpfulness_Label', labelUDF(review_df['Helpfulness_perct'])) # Apply the created UDF\n",
    "\n",
    "\n",
    "# Keep only two columns\n",
    "review_df=review_df['Helpfulness_label', 'Text']\n",
    "review_df.groupBy('Helpfulness_label').count().orderBy('Helpfulness_label').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replicating unhelpful reviews\n",
    "\n",
    "# As a result, of class inblance, logistic regression strugled to differentiate between classes, and tended to predict more frequent one.\n",
    "# Attempts were made in dealing, with class inblance, via un-/helpfulness threshold changing (<30%; >90%), as well as outlier definition (helpfulness denominator < 5). \n",
    "#In the end the choice was made , to use oversampling. Unhelpful reviews were quadrupled.\n",
    "\n",
    "from pyspark.sql.functions import array, explode, lit\n",
    "\n",
    "#Creating a dataframe with helpful reviews only. This will be used to filtered out duplicate helpful reviews.\n",
    "helpful_df = review_df[(review_df['Helpfulness_label']==1)] \n",
    "\n",
    "#Duplicate all data  and attaches Helpfulness_label ==0.\n",
    "review_df = review_df.withColumn(\"Helpfulness_label\", explode(array(lit(0),(review_df[\"Helpfulness_label\"]))))\n",
    "\n",
    "#Filtered out helpful reviews that were assigned with 0 when data was duplicated.\n",
    "review_df =review_df[((helpful_df['Helpfulness_label']==review_df['Helpfulness_label']) & (review_df['Text']==helpful_df['Text'])|((review_df['Helpfulness_label']==0) &(review_df['Text']!=helpful_df['Text'])))]\n",
    "\n",
    "#Duplicate again!\n",
    "review_df = review_df.withColumn(\"Helpfulness_label\", explode(array(lit(0),(review_df[\"Helpfulness_label\"]))))\n",
    "\n",
    "#Filtered out helpful reviews that were assigned with 0 when data was duplicated.\n",
    "review_df =review_df[((helpful_df['Helpfulness_label']==review_df['Helpfulness_label']) & (review_df['Text']==helpful_df['Text'])|((review_df['Helpfulness_label']==0) &(review_df['Text']!=helpful_df['Text'])))]\n",
    "\n",
    "# TESTING DUPLICATION\n",
    "#review_df.groupBy('Helpfulness_label').count().orderBy('Helpfulness_label').show() \n",
    "#review_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Renaming columns for pipeline format\n",
    "review_df = review_df.selectExpr(\"Helpfulness_label as label\", 'Text as Text') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, IDF, CountVectorizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1st Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy training data, pipeline 1 (AOC-ROC):  0.7983221662251336\n",
      "Duration of training/validation, pipeline 1: 36.028708696365356s\n"
     ]
    }
   ],
   "source": [
    "## TRAINING/VALIDATION SPLIT ESTIMATOR (PIPELINE 1, FULL DATA)\n",
    "\n",
    "from time import time\n",
    "\n",
    "training_df, test_df = review_df.randomSplit([0.8, 0.2]) # Split data into training/testing\n",
    "\n",
    "t1=time()\n",
    "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"words\") # Tokenizer\n",
    "Cvectorizer = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol=\"featuresTF\") # Feature extractor\n",
    "idf = IDF(inputCol=Cvectorizer.getOutputCol(), outputCol=\"features\") # Transformer\n",
    "lr = LogisticRegression() # Transformer\n",
    "pipeline_est = Pipeline(stages=[tokenizer ,Cvectorizer, idf, lr]) # Estimator\n",
    "\n",
    "bc_eval=BinaryClassificationEvaluator() # Evaluator, measures ROC-AUC by default.\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1]).addGrid(lr.maxIter, [50]).addGrid(Cvectorizer.vocabSize, [100]).build() # Default parameters with 0.1 regularization parameter.\n",
    "\n",
    "TVS_est = TrainValidationSplit(estimator=pipeline_est, evaluator=bc_eval, estimatorParamMaps=paramGrid, trainRatio=0.8) # Set the splited esimator training/validation 80/20.\n",
    "TVS_mod = TVS_est.fit(training_df) # Best model (ONLY MODEL)\n",
    "prediction_train = TVS_mod.transform(training_df)\n",
    "t2=time()\n",
    "\n",
    "print(\"Accuracy training data, pipeline 1 (AOC-ROC): \", bc_eval.evaluate(prediction_train))\n",
    "print('Duration of training/validation, pipeline 1: {}s'.format(t2-t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7264942016057092\n",
      "Precision positive class: 0.7465475223395613 , zero class: 0.6877615062761506\n",
      "Recall positive class: 0.8220035778175313 , zero class: 0.5841848067525545\n",
      "F1 score positive: 0.7824606215410813, zero class: 0.5122942090895731\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION (PIPELINE 1)\n",
    "\n",
    "prediction_test = TVS_mod.transform(test_df) # Get predicitons on test data\n",
    "\n",
    "predictionAndLabels=prediction_test['label', 'prediction'] # Keep only two columns for evaluation ture label vs predicted label\n",
    "\n",
    "TP_DF=predictionAndLabels[(predictionAndLabels['label']==1) & (predictionAndLabels['prediction']==1)].count() #True positive\n",
    "FP_DF=predictionAndLabels[(predictionAndLabels['label']==0) & (predictionAndLabels['prediction']==1)].count() #False positive\n",
    "TN_DF=predictionAndLabels[(predictionAndLabels['label']==0) & (predictionAndLabels['prediction']==0)].count() #True negative\n",
    "FN_DF=predictionAndLabels[(predictionAndLabels['label']==1) & (predictionAndLabels['prediction']==0)].count() #False negative\n",
    "\n",
    "Accuracy=(TP_DF+TN_DF)/(TP_DF+TN_DF+FP_DF+FN_DF) # Accuracy\n",
    "Precision_Positive=TP_DF/(TP_DF+FP_DF) # Precision for positive class\n",
    "Precision_Negative=TN_DF/(TN_DF+FN_DF) # Precision for negative class\n",
    "Recall_Positive=TP_DF/(TP_DF+FN_DF) # Recall for positive class\n",
    "Recall_Negative=TN_DF/(TN_DF+FP_DF) # Recall for negative clas\n",
    "F1_positive=2*(Precision_Positive*Recall_Positive)/(Recall_Positive+Precision_Positive) # F1 score for positive class\n",
    "F1_negative=2*(Precision_Negative*Recall_Negative)/(Precision_Positive+Recall_Positive) # F1 score for negative class\n",
    "\n",
    "print('Accuracy: {}'.format(Accuracy))\n",
    "print('Precision positive class: {} , zero class: {}'.format(Precision_Positive, Precision_Negative))\n",
    "print('Recall positive class: {} , zero class: {}'.format(Recall_Positive, Recall_Negative))\n",
    "print('F1 score positive: {}, zero class: {}'.format(F1_positive, F1_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy training data, pipeline 2 (AOC-ROC):  0.9181437209256595\n",
      "Duration of training/validation, pipeline 2: 3353.651606321335s\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION ESTIMATOR (PIPELINE 2)\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "training_df2, test_df2 = review_df.randomSplit([0.8, 0.2])# Spliting data\n",
    "\n",
    "t3=time()\n",
    "tokenizer_2 = Tokenizer(inputCol=\"Text\", outputCol=\"words\") # Tokenizer\n",
    "remover = StopWordsRemover(inputCol=tokenizer_2.getOutputCol(), outputCol=\"filtered\") # Stopword remover (NEW)\n",
    "Cvectorizer_2 = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"featuresTF\") # Feature creation with hashing\n",
    "idf_2 = IDF(inputCol=Cvectorizer_2.getOutputCol(), outputCol=\"features\") # TF.IDF\n",
    "lr_2 = LogisticRegression() #Logistic regression\n",
    "pipeline_est2 = Pipeline(stages=[tokenizer_2,remover, Cvectorizer_2, idf_2, lr_2]) # Estimator\n",
    "\n",
    "bc_eval2=BinaryClassificationEvaluator() # Evaluator\n",
    "paramGrid2 = ParamGridBuilder().addGrid(lr_2.regParam, [0.01, 0.1, 1]).addGrid(lr_2.maxIter, [10, 50, 100]).addGrid(Cvectorizer_2.vocabSize, [10, 100, 1000]).build() # Parameter grid\n",
    "\n",
    "crossval_est=CrossValidator(estimator=pipeline_est2, estimatorParamMaps=paramGrid2, evaluator=bc_eval2, numFolds=10) # 10 fold cross validation esimator\n",
    "cv_Model = crossval_est.fit(training_df2) # Best model\n",
    "prediction_train2 = cv_Model.transform(training_df2)\n",
    "t4=time()\n",
    "\n",
    "print(\"Accuracy training data, pipeline 2 (AOC-ROC): \", bc_eval2.evaluate(prediction_train2))\n",
    "print('Duration of training/validation, pipeline 2: {}s'.format(t4-t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, pipeline 2: 0.8007174887892377\n",
      "Precision, pipeline 2, positive class: 0.8472222222222222 , zero class: 0.7326557666813964\n",
      "Recall, pipeline 2, positive class: 0.8226326590442685 , zero class: 0.7661737523105361\n",
      "F1 score, pipeline 2, positive: 0.8347463929793246, zero class: 0.6723238338944666\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION (PIPELINE 2)\n",
    "prediction_test2 = cv_Model.transform(test_df2)\n",
    "\n",
    "predictionAndLabels=prediction_test2['label', 'prediction'] # Keep only two columns for evaluation ture label vs predicted label\n",
    "\n",
    "TP_DF=predictionAndLabels[(predictionAndLabels['label']==1) & (predictionAndLabels['prediction']==1)].count() #True positive\n",
    "FP_DF=predictionAndLabels[(predictionAndLabels['label']==0) & (predictionAndLabels['prediction']==1)].count() #False positive\n",
    "TN_DF=predictionAndLabels[(predictionAndLabels['label']==0) & (predictionAndLabels['prediction']==0)].count() #True negative\n",
    "FN_DF=predictionAndLabels[(predictionAndLabels['label']==1) & (predictionAndLabels['prediction']==0)].count() #False negative\n",
    "\n",
    "Accuracy=(TP_DF+TN_DF)/(TP_DF+TN_DF+FP_DF+FN_DF) # Accuracy\n",
    "Precision_Positive=TP_DF/(TP_DF+FP_DF) # Precision for positive class\n",
    "Precision_Negative=TN_DF/(TN_DF+FN_DF) # Precision for negative class\n",
    "Recall_Positive=TP_DF/(TP_DF+FN_DF) # Recall for positive class\n",
    "Recall_Negative=TN_DF/(TN_DF+FP_DF) # Recall for negative clas\n",
    "F1_positive=2*(Precision_Positive*Recall_Positive)/(Recall_Positive+Precision_Positive) # F1 score for positive class\n",
    "F1_negative=2*(Precision_Negative*Recall_Negative)/(Precision_Positive+Recall_Positive) # F1 score for negative class\n",
    "\n",
    "print('Accuracy, pipeline 2: {}'.format(Accuracy))\n",
    "print('Precision, pipeline 2, positive class: {} , zero class: {}'.format(Precision_Positive, Precision_Negative))\n",
    "print('Recall, pipeline 2, positive class: {} , zero class: {}'.format(Recall_Positive, Recall_Negative))\n",
    "print('F1 score, pipeline 2, positive: {}, zero class: {}'.format(F1_positive, F1_negative))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({Param(parent='LogisticRegression_463980565906a8c07060', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='CountVectorizer_48249614a40f8a49a4c0', name='vocabSize', doc='max size of the vocabulary. Default 1 << 18.'): 1000, Param(parent='LogisticRegression_463980565906a8c07060', name='regParam', doc='regularization parameter (>= 0).'): 0.01}, 8.932434716819103)\n"
     ]
    }
   ],
   "source": [
    "# BEST PARAMETERS\n",
    "\n",
    "paramMap=list(zip(cv_Model.getEstimatorParamMaps(), cv_Model.avgMetrics)) # Map parameters, with average metrics\n",
    "paramMax=max(paramMap, key=lambda x: x[1]) # The the parameters, with maximum values based on average metrics\n",
    "print(paramMax) # Print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Training data/UNHELPFUL REVIEWS\n",
    "\n",
    "prediction_train2.createOrReplaceTempView('final_train_df') # Register the table\n",
    "SQL_0='SELECT label, filtered, features FROM final_train_df WHERE label==0' # SQL query to leave UNHELPFUL REVIEWS\n",
    "SQL_train_0=spark.sql(SQL_0) # Get UNHELPFUL reviews dataframe\n",
    "\n",
    "Cvectorizer_0=CountVectorizer(inputCol='filtered', outputCol='vectors', vocabSize=1000) # 1 step create vocabulary (best size)\n",
    "vocabulary_0 = Cvectorizer_0.fit(SQL_train_0).vocabulary # 2 step create vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpacklists(x):\n",
    "    x1, x2=x\n",
    "    newlist=[]\n",
    "    if len(x1)==len(x2):\n",
    "        for i in range(len(x1)):\n",
    "            newlist.append((x1[i],x2[i]))\n",
    "    else:\n",
    "        print('Word list != TF.IDF list')\n",
    "    return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 25 terms for TRAIN DATA/UNHELPFULL REVIEWS \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('', 61148.64659787561),\n",
       " ('/><br', 28604.208956306662),\n",
       " ('like', 6580.1790703728657),\n",
       " ('bags', 4462.9505104399741),\n",
       " ('taste', 4201.1463931012458),\n",
       " ('good', 3810.374888460929),\n",
       " ('much', 2287.3237676207455),\n",
       " ('try', 1634.5649279682625),\n",
       " ('us', 1613.466946905728),\n",
       " ('something', 1517.8881027582836),\n",
       " ('give', 1358.755589770185),\n",
       " ('say', 1276.9888102650214),\n",
       " ('would', 1235.3370499928162),\n",
       " ('however,', 1025.0455407480329),\n",
       " ('one', 976.52909909644995),\n",
       " ('baby', 771.59632420282401),\n",
       " (\"it's\", 767.40236691715802),\n",
       " ('milk', 752.66701822286734),\n",
       " ('product', 751.90723645696619),\n",
       " ('it.<br', 700.0025735796537),\n",
       " ('get', 533.56841592524358),\n",
       " ('buy', 524.42148663521516),\n",
       " ('says', 484.89410667982054),\n",
       " ('cheaper', 480.33221802943621),\n",
       " (\"don't\", 472.87959840949833)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNHELP_TRAIN_RDD= SQL_train_0.rdd.map(lambda x: ([w for w in x['filtered'] if w in vocabulary_0], [x['features'][vocabulary_0.index(w)] for w in x['filtered'] if w in vocabulary_0]))\n",
    "UNHELP_TRAIN_RDD2=UNHELP_TRAIN_RDD.flatMap(lambda x: unpacklists(x)) \n",
    "UNHELP_TRAIN_RDD3=UNHELP_TRAIN_RDD2.reduceByKey(lambda a,b: a+b) # REDUCE by adding TF.IDF values.\n",
    "\n",
    "print('TOP 25 terms for TRAIN DATA/UNHELPFULL REVIEWS \\n')\n",
    "UNHELP_TRAIN_RDD3.sortBy(lambda a: a[1], ascending=False).take(25) # Take top 25 most important words/terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Training data/HELPFUL REVIEWS\n",
    "\n",
    "SQL_1='SELECT label, filtered, features FROM final_train_df WHERE label==1' # SQL query to leave HELPFUL REVIEWS\n",
    "SQL_train_1=spark.sql(SQL_1) # Get HELPFUL reviews dataframe\n",
    "\n",
    "Cvectorizer_1=CountVectorizer(inputCol='filtered', outputCol='vectors', vocabSize=1000) # 1 step create vocabulary\n",
    "vocabulary_1 = Cvectorizer_1.fit(SQL_train_1).vocabulary # 2 step create vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 25 terms for TRAIN DATA/HELPFULL REVIEWS \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('', 203819.79802735956),\n",
       " ('/><br', 74836.919605018164),\n",
       " ('coffee', 30652.178770446109),\n",
       " ('like', 16542.342633498556),\n",
       " ('dog', 14854.644743638695),\n",
       " ('one', 13991.010111496938),\n",
       " ('would', 11089.030436406483),\n",
       " ('taste', 10016.690914293471),\n",
       " ('really', 9317.8796913897349),\n",
       " ('much', 8022.9151884723651),\n",
       " ('time', 5996.1203812019521),\n",
       " ('try', 4724.7993719982023),\n",
       " ('since', 4641.2424109675558),\n",
       " ('without', 4317.0192447807613),\n",
       " ('good', 3968.2456858418159),\n",
       " ('use', 2578.9435606011048),\n",
       " (\"it's\", 2467.2896149171575),\n",
       " ('-', 2384.4114863058021),\n",
       " ('get', 2373.535762065067),\n",
       " ('times', 2318.5023666671068),\n",
       " ('tried', 2285.2827710186707),\n",
       " ('probably', 2264.349821668699),\n",
       " ('little', 1964.8760641943354),\n",
       " ('food', 1944.7099005534681),\n",
       " ('product', 1847.5619515012852)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HELP_TRAIN_RDD= SQL_train_1.rdd.map(lambda x: ([w for w in x['filtered'] if w in vocabulary_1], [x['features'][vocabulary_1.index(w)] for w in x['filtered'] if w in vocabulary_1]))\n",
    "HELP_TRAIN_RDD2=HELP_TRAIN_RDD.flatMap(lambda x: unpacklists(x)) \n",
    "HELP_TRAIN_RDD3=HELP_TRAIN_RDD2.reduceByKey(lambda a,b: a+b) # REDUCE by adding TF.IDF values.\n",
    "\n",
    "print('TOP 25 terms for TRAIN DATA/HELPFULL REVIEWS \\n')\n",
    "HELP_TRAIN_RDD3.sortBy(lambda a: a[1], ascending=False).take(25) # Take top 25 most important words/terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Testing data/UNHELPFUL REVIEWS\n",
    "\n",
    "prediction_test2.createOrReplaceTempView('final_test_df') # Register the table\n",
    "SQL_00='SELECT label, filtered, features FROM final_test_df WHERE label==0' # SQL query to leave UNHELPFUL REVIEWS\n",
    "SQL_train_00=spark.sql(SQL_00) # Get UNHELPFUL reviews dataframe\n",
    "\n",
    "Cvectorizer_00=CountVectorizer(inputCol='filtered', outputCol='vectors', vocabSize=1000) # 1 step create vocabulary\n",
    "vocabulary_00 = Cvectorizer_00.fit(SQL_train_00).vocabulary # 2 step create vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 25 terms for TEST DATA/UNHELPFULL REVIEWS \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('', 17799.020061798212),\n",
       " ('/><br', 8791.2837768289319),\n",
       " ('like', 1766.6336740332458),\n",
       " ('much', 648.58336166312733),\n",
       " ('make', 555.63660346386337),\n",
       " ('good', 371.15746061154368),\n",
       " ('would', 307.98814123108593),\n",
       " ('1', 299.24245938673693),\n",
       " ('get', 245.52014095593046),\n",
       " ('one', 245.38423515756912),\n",
       " (\"don't\", 233.96413625523056),\n",
       " ('product', 201.43118195472965),\n",
       " ('eternally', 186.29121167860433),\n",
       " ('buy', 175.5361163134632),\n",
       " ('covering', 166.11907788255613),\n",
       " ('us', 154.14603821226157),\n",
       " (\"it's\", 139.17225242164179),\n",
       " ('taste', 134.78210775922901),\n",
       " ('sin', 132.32368532981363),\n",
       " ('ordered', 121.28135778135885),\n",
       " ('yahushua', 114.35500028760868),\n",
       " ('but,', 110.42206319671919),\n",
       " (\"i'm\", 96.872827542156813),\n",
       " ('real', 96.109041963698488),\n",
       " ('maltodextrin', 88.365067471207141)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNHELP_TEST_RDD= SQL_train_00.rdd.map(lambda x: ([w for w in x['filtered'] if w in vocabulary_00], [x['features'][vocabulary_00.index(w)] for w in x['filtered'] if w in vocabulary_00]))\n",
    "UNHELP_TEST_RDD2=UNHELP_TEST_RDD.flatMap(lambda x: unpacklists(x)) \n",
    "UNHELP_TEST_RDD3=UNHELP_TEST_RDD2.reduceByKey(lambda a,b: a+b) # REDUCE by adding TF.IDF values.\n",
    "\n",
    "print('TOP 25 terms for TEST DATA/UNHELPFULL REVIEWS \\n')\n",
    "UNHELP_TEST_RDD3.sortBy(lambda a: a[1], ascending=False).take(25) # Take top 25 most important words/terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Testing data/UNHELPFUL REVIEWS\n",
    "\n",
    "SQL_11='SELECT label, filtered, features FROM final_test_df WHERE label==1' # SQL query to leave HELPFUL REVIEWS\n",
    "SQL_train_11=spark.sql(SQL_11) # Get HELPFUL reviews dataframe\n",
    "\n",
    "Cvectorizer_11=CountVectorizer(inputCol='filtered', outputCol='vectors', vocabSize=1000) # 1 step create vocabulary\n",
    "vocabulary_11 = Cvectorizer_11.fit(SQL_train_11).vocabulary # 2 step create vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 25 terms for TEST DATA/HELPFULL REVIEWS \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('', 52017.145452478057),\n",
       " ('/><br', 21294.301154063818),\n",
       " ('like', 4417.2093208488441),\n",
       " ('one', 3390.5019558028735),\n",
       " ('taste', 2393.7627416522364),\n",
       " ('sugar', 2103.3757414952574),\n",
       " ('time', 1556.1531835690769),\n",
       " ('found', 1353.652850243634),\n",
       " ('good', 1119.0120454258463),\n",
       " ('use', 817.9386203482868),\n",
       " ('product', 754.51479109674392),\n",
       " ('also', 693.59746309876095),\n",
       " ('reviews', 673.03345094214956),\n",
       " ('really', 658.27775917144595),\n",
       " ('getting', 652.41215929125804),\n",
       " ('get', 644.21278623902731),\n",
       " ('would', 641.85933439872326),\n",
       " (\"it's\", 627.82198020182921),\n",
       " (\"don't\", 626.21841815750224),\n",
       " ('coffee', 623.12620175458392),\n",
       " ('food', 617.81606054501901),\n",
       " (\"i've\", 508.78155941760372),\n",
       " ('even', 422.81383781054063),\n",
       " ('flavor', 416.80286640939659),\n",
       " ('tried', 411.28126367322386)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HELP_TEST_RDD= SQL_train_11.rdd.map(lambda x: ([w for w in x['filtered'] if w in vocabulary_11], [x['features'][vocabulary_11.index(w)] for w in x['filtered'] if w in vocabulary_11]))\n",
    "HELP_TEST_RDD2=HELP_TEST_RDD.flatMap(lambda x: unpacklists(x)) \n",
    "HELP_TEST_RDD3=HELP_TEST_RDD2.reduceByKey(lambda a,b: a+b) # REDUCE by adding TF.IDF values.\n",
    "\n",
    "print('TOP 25 terms for TEST DATA/HELPFULL REVIEWS \\n')\n",
    "HELP_TEST_RDD3.sortBy(lambda a: a[1], ascending=False).take(25) # Take top 25 most important words/terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First pipeline returned satisfactory results, achieving 72% accuracy. It is well above 57% , which is the proportion of positive reviews, which was an initial threshold we wanted to surpass. Both, precision and recall, were higher for helpful class, indicating algorithms power in differentiating positive class. It is a little surprising, as we expected, that oversampling, will increase the discriminatory power (via duplicaton) of ''unhelpful'' features. Second pipeline, consisted of 10fold cross-validated logistic regression model, had an extra preprocessing step - stopword removal and parameter grids for logistic regression regularization parameter, maximum number of iterations, and size of vocabulary (feature) vector. It returned an accuracy of over 80%, with ROC-AUC score of over 90%, which we consider as being quite high. Precision and recall remained higher for positive class (84%; 82%), however difference was decreased, as could be seen from harmonic mean (f1 scores): helpful reviews (78% -> 83%); unhelpful (51% -> 67%). As was expected, best parameters included, the highest specified feature vector size - 1000. Which, still not huge, considering, there were ~ 80000 different term tokens, before any preprocessing. Best value for max iterations is 50, which is the middle of specified range, indicating, that model converges to optimum values effectively. Regularization parameter takes the lower bound value, (0.01) suggesting that model, is discriminating noise from signal well. Overall, it took 36 seconds to train first pipeline, and 56minutes to train the second one. Even though, time difference seems significant, final model, followed an exhaustive search over parameter grid, along with 10 fold cross-validation, which resulted in much more robust and generalizeble model. \n",
    "\n",
    "\n",
    "We defined important words, as having high TF.IDF values. It is a combined weight, consisting of term frequency measure within review (TF) and term specificity along the corpus (IDF). Dataset, was split based on label. TF.IDF values for the whole 1000 word vocabulary (1000 is our best value for the size of feature vector) were summed up. Terms at the top of the lists, were unintended spaces, and line break symbols prevalent along the whole corpus. Most important verb, was ''like'' on both lists as well. ''Bags'', was the first noun on the unhelpful reviews list, while ''Dog'' on helpful. Possibly, indicating our shared compassion for humans' best animal friends. There were a lot shared words, and a lot of, not very distinctive ones. However, we had an interesting result from deducing important words on testing data only. ''Yahushua'', was on top25 significant ''unhelpful\" words, after digging deeper we found it is a translation for name Jesus in Hebrew. There was only one review, with this term in the whole corpus. Review, was just a long religious rant, which was very unique, seen by a lot of people, and marked as helpful by very few. (very low helpfulness perct) It is interesting, that model had picked up on a term occurring in a single unhelpful review (which of course was quadrupled). Though at least in this instance, we believe it is justifiable, as food reviews section is not an expected venue for expressing religious beliefs. \n",
    "\n",
    "\n",
    "Despite, the fact that high classification accuracy was achieved on this data, it was just a glimpse on a space of infinite possibilities. First of all, data comprised only 4% of total number of reviews. Almost certainly there are certain useful patterns, hidden in unutilized data. Data quality could have been further enhanced, via removal of unintended spaces, and line break characters, which were highly prevalent and important terms. Another possible improvement, is bigger parameter grid. Only 27 different parameter combinations were tested, which is tiny number, considering, that pipeline consisted multiple parametrisable members. Finally, other models like SVMs are known to return higher accuracies, while Naive Bayes is known to be scalable, and quick at converging to minimum absolute error. It would be interesting to see how they would compare against logistic regression on this particular task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
